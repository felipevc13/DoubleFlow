This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  main.py
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/main.py">
# extraction_service/app/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import langextract as lx
import textwrap
import os
import json

app = FastAPI(title="LangExtract Analysis Service")

class ExtractionRequest(BaseModel):
    text_content: str
    kpi_data: Optional[List[Dict[str, Any]]] = None

class AnalysisResult(BaseModel):
    insights: List[Dict[str, Any]]

# Prompt atualizado: síntese qualitativo + quantitativo, com campo evidence
prompt_description = textwrap.dedent("""\
    Você é um analista de dados especialista em UX Research. Sua tarefa é sintetizar dados qualitativos (feedback de texto) e quantitativos (KPIs e distribuições) de uma pesquisa para gerar insights acionáveis.
    
    Analise ambos os conjuntos de dados para encontrar correlações, validar hipóteses e identificar as dores mais impactantes.

    Para cada insight gerado, preencha os seguintes campos:
    - 'quote': Uma citação qualitativa representativa que suporta o insight.
    - 'topic': A categoria principal (ex: Usabilidade, Performance, Preço).
    - 'sentiment': O sentimento expresso (positivo, negativo, neutro).
    - 'user_need': A necessidade do usuário por trás do feedback.
    - 'evidence': A justificativa para este insight, citando se ele é baseado em dados qualitativos, quantitativos, ou ambos. Se for quantitativo, mencione o KPI.

    REGRAS DE SAÍDA ESTRITAS:
    1. Sua resposta DEVE ser APENAS o objeto JSON.
    2. NÃO inclua nenhum texto, explicação ou comentário antes ou depois do JSON.
    3. Garanta que todas as strings dentro do JSON, especialmente no campo 'quote', tenham aspas duplas (") devidamente escapadas com uma contrabarra (\\").
""")

# Exemplo híbrido (qualitativo + quantitativo)
examples = [
    lx.data.ExampleData(
        text=textwrap.dedent("""\
            --- DADOS QUALITATIVOS ---
            O aplicativo é muito lento para carregar os relatórios, fico esperando uma eternidade.
            Amo a nova funcionalidade de exportar para PDF! Me economiza muito tempo.

            --- DADOS QUANTITATIVOS ---
            [
                {
                    "metric": "NPS (Net Promoter Score)",
                    "value": "4.1",
                    "details": "N=50 respostas",
                    "distribution": {"1": 5, "2": 10, "3": 5, "4": 10, "5": 20}
                }
            ]
        """),
        extractions=[
            lx.data.Extraction(
                extraction_class="insight",
                extraction_text="O aplicativo é muito lento para carregar os relatórios, fico esperando uma eternidade.",
                attributes={
                    "topic": "Performance",
                    "sentiment": "negativo",
                    "user_need": "O usuário precisa de acesso rápido aos dados para ser produtivo.",
                    "evidence": "Qualitativo. A lentidão mencionada pode ser uma causa para o baixo score de NPS (4.1)."
                }
            ),
            lx.data.Extraction(
                extraction_class="insight",
                extraction_text="Amo a nova funcionalidade de exportar para PDF! Me economiza muito tempo.",
                attributes={
                    "topic": "Funcionalidades",
                    "sentiment": "positivo",
                    "user_need": "O usuário precisa de maneiras eficientes para compartilhar relatórios.",
                    "evidence": "Qualitativo. Este é um ponto forte que contribui positivamente para o NPS."
                }
            )
        ]
    )
]

@app.post("/extract", response_model=AnalysisResult)
async def extract_data(request: ExtractionRequest):
    qualitative_content = request.text_content.strip()
    quantitative_content = ""

    # Formata o kpi_data, se existir, como JSON legível para a IA
    if request.kpi_data:
        try:
            kpi_json_string = json.dumps(request.kpi_data, indent=2, ensure_ascii=False)
            quantitative_content = f"---\nDADOS QUANTITATIVOS ---\n{kpi_json_string}"
        except TypeError:
            print("Aviso: kpi_data não pôde ser serializado para JSON.")

    # Une os blocos qualitativo e quantitativo
    text_to_analyze = f"---\nDADOS QUALITATIVOS ---\n{qualitative_content}\n\n{quantitative_content}".strip()

    if not text_to_analyze or text_to_analyze == "---\nDADOS QUALITATIVOS ---\n\n---DADOS QUANTITATIVOS ---":
        raise HTTPException(status_code=400, detail="Nenhum conteúdo válido (qualitativo ou quantitativo) foi fornecido.")

    try:
        result = lx.extract(
            text_or_documents=text_to_analyze,
            prompt_description=prompt_description,
            examples=examples,
            model_id=os.getenv("LANGEXTRACT_MODEL_ID", "gemini-1.5-flash-latest")
        )

        insights = [
            {
                "quote": e.extraction_text,
                "topic": e.attributes.get("topic", "Geral"),
                "sentiment": e.attributes.get("sentiment", "neutro"),
                "user_need": e.attributes.get("user_need", "Não especificada"),
                "evidence": e.attributes.get("evidence", "Qualitativo")
            }
            for e in result.extractions
        ]

        return {"insights": insights}

    except Exception as e:
        print(f"Erro ao chamar LangExtract: {e}")
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="README.md">
# LangExtract Analysis Service

A FastAPI microservice for extracting and analyzing text data using LangExtract.

## Setup

1. Create and activate a virtual environment:

   ```bash
   # Recomenda-se criar o ambiente virtual FORA da pasta do projeto frontend (Nuxt) para evitar conflitos de watchers.
   python3 -m venv ~/doubleflow-langextract-env
   source ~/doubleflow-langextract-env/bin/activate  # No Windows: %USERPROFILE%\doubleflow-langextract-env\Scripts\activate
   ```

   > **Observação:** Evite criar ambientes virtuais dentro de projetos frontend para não causar problemas de watchers (como EMFILE: too many open files).

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

## Running the Service

1. Start the FastAPI development server:

   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

2. The service will be available at `http://localhost:8000`

3. API documentation will be available at:
   - Swagger UI: `http://localhost:8000/docs`
   - ReDoc: `http://localhost:8000/redoc`

## Environment Variables

Create a `.env` file in the root directory with the following variables:

```
# API Configuration
API_KEY=your_api_key_here
DEBUG=true
```

## API Endpoints

- `GET /health`: Health check endpoint
- `POST /extract`: Extract insights from text

## Development

### Testing

To run tests:

```bash
pytest
```

### Linting

```bash
flake8 .
black .
isort .
```

## Deployment

For production deployment, consider using:

- Gunicorn with Uvicorn workers
- Docker
- Kubernetes
- AWS ECS/EKS
- Google Cloud Run
- Azure Container Instances
</file>

<file path="requirements.txt">
fastapi
uvicorn[standard]
pydantic
python-dotenv
langextract
google-generativeai
# Adicione outras dependências se necessário, como 'google-generativeai'
</file>

</files>
